{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.datasets import cifar10, mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras functional api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сеть в столбик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "# Создаем код для нашей модели\n",
    "# входящий слой формат картинки 32 на 32 3х канального цвета\n",
    "input = keras.Input(shape=(32,32,3))\n",
    "# Conv2D - слой свертки по изображениям\n",
    "x = layers.Conv2D(32, 3, activation='relu')(input)\n",
    "x = layers.MaxPool2D(2, padding='same')(x)\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.MaxPool2D(2, padding='same')(x)\n",
    "# слой для выравнивания выходных данных\n",
    "x = layers.Flatten()(x)\n",
    "# Dense - слой полносвязной сети\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "# выкидывает какие-то значения на выходе\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output = layers.Dense(10, activation='softmax')(x)\n",
    "# на основе кода сверху генерируем модель с помощью:\n",
    "model = keras.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Показываем нашу модеь изнутри"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               803072    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 825,034\n",
      "Trainable params: 825,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализуем наш датасет\n",
    "x_train = x_train /255\n",
    "x_test = x_test/255\n",
    "#  приводим к категориальному формату\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "# Добавляем оптимизатор, функцию потерь и метрику качества в нашу модель\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "625/625 [==============================] - 22s 34ms/step - loss: 1.5874 - accuracy: 0.4220 - val_loss: 1.2809 - val_accuracy: 0.5588\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 1.2501 - accuracy: 0.5557 - val_loss: 1.0974 - val_accuracy: 0.6136\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 21s 34ms/step - loss: 1.1027 - accuracy: 0.6116 - val_loss: 1.0818 - val_accuracy: 0.6196\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 21s 34ms/step - loss: 0.9961 - accuracy: 0.6497 - val_loss: 0.9314 - val_accuracy: 0.6726\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 21s 34ms/step - loss: 0.9245 - accuracy: 0.6749 - val_loss: 0.8939 - val_accuracy: 0.6881\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.8675 - accuracy: 0.6961 - val_loss: 0.9138 - val_accuracy: 0.6864\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.8088 - accuracy: 0.7150 - val_loss: 0.8339 - val_accuracy: 0.7113\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 20s 33ms/step - loss: 0.7535 - accuracy: 0.7342 - val_loss: 0.8988 - val_accuracy: 0.6961\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.7031 - accuracy: 0.7486 - val_loss: 0.8274 - val_accuracy: 0.7189\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 21s 34ms/step - loss: 0.6592 - accuracy: 0.7642 - val_loss: 0.8210 - val_accuracy: 0.7180\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 21s 34ms/step - loss: 0.6205 - accuracy: 0.7791 - val_loss: 0.8187 - val_accuracy: 0.7272\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 21s 34ms/step - loss: 0.5832 - accuracy: 0.7930 - val_loss: 0.8329 - val_accuracy: 0.7253\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.5459 - accuracy: 0.8053 - val_loss: 0.8705 - val_accuracy: 0.7170\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.5093 - accuracy: 0.8155 - val_loss: 0.8857 - val_accuracy: 0.7147\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 20s 33ms/step - loss: 0.4834 - accuracy: 0.8256 - val_loss: 0.8662 - val_accuracy: 0.7245\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.4522 - accuracy: 0.8367 - val_loss: 0.8616 - val_accuracy: 0.7269\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.4212 - accuracy: 0.8458 - val_loss: 0.9284 - val_accuracy: 0.7224\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 22s 35ms/step - loss: 0.4006 - accuracy: 0.8552 - val_loss: 0.9461 - val_accuracy: 0.7171\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.3786 - accuracy: 0.8610 - val_loss: 0.9428 - val_accuracy: 0.7295\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.3614 - accuracy: 0.8689 - val_loss: 0.9553 - val_accuracy: 0.7272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d8df168b50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучаем модель\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.9680 - accuracy: 0.7241\n",
      "[0.9679670929908752, 0.7240999937057495]\n"
     ]
    }
   ],
   "source": [
    "# Вычисляем точность нашей модели\n",
    "print(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сеть с помощью класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделали класс сверточного слоя\n",
    "class TfConv2D(tf.Module):\n",
    "    def __init__(self, kernel=(3,3), channels=1, strides=(2,2), padding='SAME', activation='relu'):\n",
    "        super().__init__()\n",
    "        self.kernel = kernel        # Размер ядра\n",
    "        self.channels = channels    # Число выходных каналов\n",
    "        self.strides = strides      # Смещение в плоскости канала\n",
    "        self.padding = padding      # режим формирования выходных каналов\n",
    "        self.activate = activation  # функция активации\n",
    "        self.fl_init=False\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not self.fl_init:\n",
    "            #  формируем весовые кофициенты под формат\n",
    "            # [kernel_x, kernel_y, input_channels, output_channels]\n",
    "            # truncated_normal - нормальные случайные величины, со средне квадратическим отклонением - stddev=0.1\n",
    "            self.w = tf.random.truncated_normal((*self.kernel, x.shape[-1], self.channels), stddev=0.1, dtype=tf.double)\n",
    "            self.b = tf.zeros([self.channels], dtype=tf.double)\n",
    "\n",
    "            self.w = tf.Variable(self.w)\n",
    "            self.b = tf.Variable(self.b)\n",
    "\n",
    "            self.fl_init = True\n",
    "        y = tf.nn.conv2d(x, self.w, strides=(1, *self.strides, 1), padding=self.padding)+self.b\n",
    "\n",
    "        if self.activate == 'relu':\n",
    "            return tf.nn.relu(y)\n",
    "        elif self.activate == 'softmax':\n",
    "            return tf.nn.softmax(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 16, 32)\n",
      "(1, 8, 8, 32)\n"
     ]
    }
   ],
   "source": [
    "layer1 = TfConv2D((3, 3), 32)\n",
    "y = layer1(tf.expand_dims(x_test[0], axis=0))\n",
    "print(y.shape)\n",
    "y = tf.nn.max_pool2d(y, ksize=(1,2,2,1), strides=(1,2,2,1), padding=\"SAME\")\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель автоинкодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input = layers.Input(shape=(28,28,1))\n",
    "# Conv2D - слой свертки по изображениям\n",
    "x = layers.Conv2D(32, 3, activation='relu')(enc_input)\n",
    "x = layers.MaxPool2D(2, padding='same')(x)\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.MaxPool2D(2, padding='same')(x)\n",
    "# слой для выравнивания выходных данных\n",
    "x = layers.Flatten()(x)\n",
    "enc_output = layers.Dense(8, activation='linear')(x)\n",
    "encoder = keras.Model(enc_input, enc_output, name=\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_input = keras.Input(shape=(8,), name='encoded_img')\n",
    "x = layers.Dense(7*7*8, activation='relu')(dec_input)\n",
    "x = keras.layers.Reshape((7, 7, 8))(x)\n",
    "x = layers.Conv2DTranspose(64, 5, strides=(2,2), activation='relu', padding='same')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = layers.Conv2DTranspose(32, 5, strides=(2,2), activation=\"linear\", padding='same')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "dec_output = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n",
    "decoder = keras.Model(dec_input, dec_output, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_input = keras.Input(shape=(28, 28, 1), name='img')\n",
    "x = encoder(autoencoder_input)\n",
    "autoencoder_output = decoder(x)\n",
    "\n",
    "autoencoder = keras.Model(autoencoder_input, autoencoder_output, name='autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Нормализуем наш датасет\n",
    "x_train = x_train.astype('float32') /255.0\n",
    "x_test = x_test.astype('float32')/255.0\n",
    "#  приводим к категориальному формату\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 82s 43ms/step - loss: 0.0300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d8ea4bc4f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train, batch_size=32, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n"
     ]
    }
   ],
   "source": [
    "h = encoder.predict(tf.expand_dims(x_test[0], axis=0))\n",
    "img = decoder.predict(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(121)\n",
    "plt.imshow(x_test[0], cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(img.squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet by hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train /255\n",
    "x_test = x_test/255\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(32, 32, 3), name='img')\n",
    "x = layers.Conv2D(32, 3, activation='relu')(inputs)\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "block_1_output = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(64, 3, activation='relu', padding='same')(block_1_output)\n",
    "x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_2_output = layers.add([x, block_1_output])\n",
    "x = layers.Conv2D(64, 3, activation='relu', padding='same')(block_2_output)\n",
    "x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_3_output = layers.add([x, block_2_output])\n",
    "x = layers.Conv2D(64, 3, activation='relu')(block_3_output)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs, name='toy_resnet')\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "625/625 [==============================] - 62s 96ms/step - loss: 1.8198 - accuracy: 0.2907 - val_loss: 1.5030 - val_accuracy: 0.4366\n",
      "Epoch 2/15\n",
      "625/625 [==============================] - 59s 94ms/step - loss: 1.3801 - accuracy: 0.4895 - val_loss: 1.2164 - val_accuracy: 0.5520\n",
      "Epoch 3/15\n",
      "625/625 [==============================] - 58s 92ms/step - loss: 1.1343 - accuracy: 0.5881 - val_loss: 1.0367 - val_accuracy: 0.6291\n",
      "Epoch 4/15\n",
      "625/625 [==============================] - 58s 93ms/step - loss: 0.9664 - accuracy: 0.6516 - val_loss: 0.9046 - val_accuracy: 0.6817\n",
      "Epoch 5/15\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.8618 - accuracy: 0.6914 - val_loss: 0.8476 - val_accuracy: 0.6980\n",
      "Epoch 6/15\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.7821 - accuracy: 0.7220 - val_loss: 0.8529 - val_accuracy: 0.6989\n",
      "Epoch 7/15\n",
      "625/625 [==============================] - 58s 93ms/step - loss: 0.7145 - accuracy: 0.7498 - val_loss: 0.7377 - val_accuracy: 0.7392\n",
      "Epoch 8/15\n",
      "625/625 [==============================] - 60s 96ms/step - loss: 0.6612 - accuracy: 0.7707 - val_loss: 0.7104 - val_accuracy: 0.7503\n",
      "Epoch 9/15\n",
      "625/625 [==============================] - 59s 95ms/step - loss: 0.6036 - accuracy: 0.7896 - val_loss: 0.6844 - val_accuracy: 0.7686\n",
      "Epoch 10/15\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.5577 - accuracy: 0.8066 - val_loss: 0.6897 - val_accuracy: 0.7663\n",
      "Epoch 11/15\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.5171 - accuracy: 0.8219 - val_loss: 0.6557 - val_accuracy: 0.7777\n",
      "Epoch 12/15\n",
      "625/625 [==============================] - 61s 97ms/step - loss: 0.4834 - accuracy: 0.8323 - val_loss: 0.6655 - val_accuracy: 0.7803\n",
      "Epoch 13/15\n",
      "625/625 [==============================] - 59s 94ms/step - loss: 0.4411 - accuracy: 0.8477 - val_loss: 0.6870 - val_accuracy: 0.7775\n",
      "Epoch 14/15\n",
      "625/625 [==============================] - 61s 98ms/step - loss: 0.4095 - accuracy: 0.8591 - val_loss: 0.6899 - val_accuracy: 0.7791\n",
      "Epoch 15/15\n",
      "625/625 [==============================] - 62s 99ms/step - loss: 0.3801 - accuracy: 0.8673 - val_loss: 0.7091 - val_accuracy: 0.7780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f41d6581c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=64, epochs=15, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 12ms/step - loss: 0.7336 - accuracy: 0.7706\n",
      "[0.7336087226867676, 0.7706000208854675]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs):\n",
    "        self.per_batch_losses = []\n",
    "    \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.per_batch_losses.append(logs.get('loss'))\n",
    "\n",
    "    def on_train_end(self, logs):\n",
    "        print(self.per_batch_losses[:5])\n",
    "callbacks = [\n",
    "    CustomCallback(),\n",
    "    # ранняя остановка чтобы избежать проблем\n",
    "    # keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.01, patience=0.01, patience=2, verbose=1),\n",
    "    # чекпоинты модели - сохранение модели на разных этапах\n",
    "    # keras.callbacks.ModelCheckpoint(filepath=\"mymodel_{epoch}\", save_best_only=True, monitor='loss', verbose=1)\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 80s 50ms/step - loss: 0.5443 - accuracy: 0.8157\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 79s 50ms/step - loss: 0.4811 - accuracy: 0.8365\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 78s 50ms/step - loss: 0.4417 - accuracy: 0.8478\n",
      "[0.33254474401474, 0.27401456236839294, 0.2993761897087097, 0.3283223509788513, 0.3480733335018158]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f41d6a6aa0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=3, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
